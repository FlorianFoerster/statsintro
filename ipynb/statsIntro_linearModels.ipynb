{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Linear Modeling in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook introduces the use of pandas and the formula framework in statsmodels in the context of linear modeling."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**It is based heavily on Jonathan Taylor's [class notes that use R](http://www.stanford.edu/class/stats191/interactions.html)**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import numpy as np\n",
      "\n",
      "from statsmodels.formula.api import ols\n",
      "from statsmodels.graphics.api import interaction_plot, abline_plot, qqplot\n",
      "from statsmodels.stats.api import anova_lm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 1: IT salary data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example we will first establish a linear model, of salary as a function of experience, education, and management level.\n",
      "We will test for any interactions between these factors. Significant interactions will be included in the model.\n",
      "Finally, we will remove outliers, and plot the resulting fits."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Get the data"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Outcome:    S, salaries for IT staff in a corporation\n",
      "Predictors: X, experience in years\n",
      "            M, managment, 2 levels, 0=non-management, 1=management\n",
      "            E, education, 3 levels, 1=Bachelor's, 2=Master's, 3=Ph.D"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://stats191.stanford.edu/data/salary.table'\n",
      "salary_table = pandas.read_table(url) # needs pandas 0.7.3\n",
      "#salary_table.to_csv('salary.table', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Inspect the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(salary_table.head(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "       S  X  E  M\n",
        "0  13876  1  1  1\n",
        "1  11608  1  3  0\n",
        "2  18701  1  3  1\n",
        "3  11283  1  2  0\n",
        "4  11767  1  3  0\n",
        "5  20872  2  2  1\n",
        "6  11772  2  2  0\n",
        "7  10535  2  1  0\n",
        "8  12195  2  3  0\n",
        "9  12313  3  2  0\n",
        "\n",
        "[10 rows x 4 columns]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "E = salary_table.E # Education\n",
      "M = salary_table.M # Management\n",
      "X = salary_table.X # Experience\n",
      "S = salary_table.S # Salary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's explore the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(10,8))\n",
      "ax = fig.add_subplot(111, xlabel='Experience', ylabel='Salary',\n",
      "            xlim=(0, 27), ylim=(9600, 28800))\n",
      "symbols = ['D', '^']\n",
      "man_label = [\"Non-Mgmt\", \"Mgmt\"]\n",
      "educ_label = [\"Bachelors\", \"Masters\", \"PhD\"]\n",
      "colors = ['r', 'g', 'blue']\n",
      "factor_groups = salary_table.groupby(['E','M'])\n",
      "for values, group in factor_groups:\n",
      "    i,j = values\n",
      "    label = \"%s - %s\" % (man_label[j], educ_label[i-1])\n",
      "    ax.scatter(group['X'], group['S'], marker=symbols[j], color=colors[i-1],\n",
      "               s=350, label=label)\n",
      "ax.legend(scatterpoints=1, markerscale=.7, labelspacing=1);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Define and Fit a Linear Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fit a linear model\n",
      "\n",
      "$$S_i = \\beta_0 + \\beta_1X_i + \\beta_2E_{i2} + \\beta_3E_{i3} + \\beta_4M_i + \\epsilon_i$$\n",
      "\n",
      "where\n",
      "\n",
      "$$ E_{i2}=\\cases{1,&if $E_i=2$;\\cr 0,&otherwise. \\cr}$$ \n",
      "$$ E_{i3}=\\cases{1,&if $E_i=3$;\\cr 0,&otherwise. \\cr}$$ \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the following, the model is defined using \"patsy\".\n",
      "\n",
      "- An intercept is automatically included.\n",
      "- C(variable) includes the variable automatically as categories."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "formula = 'S ~ C(E) + C(M) + X'\n",
      "lm = ols(formula, salary_table).fit()\n",
      "print(lm.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                      S   R-squared:                       0.957\n",
        "Model:                            OLS   Adj. R-squared:                  0.953\n",
        "Method:                 Least Squares   F-statistic:                     226.8\n",
        "Date:                Tue, 25 Feb 2014   Prob (F-statistic):           2.23e-27\n",
        "Time:                        15:10:10   Log-Likelihood:                -381.63\n",
        "No. Observations:                  46   AIC:                             773.3\n",
        "Df Residuals:                      41   BIC:                             782.4\n",
        "Df Model:                           4                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept   8035.5976    386.689     20.781      0.000      7254.663  8816.532\n",
        "C(E)[T.2]   3144.0352    361.968      8.686      0.000      2413.025  3875.045\n",
        "C(E)[T.3]   2996.2103    411.753      7.277      0.000      2164.659  3827.762\n",
        "C(M)[T.1]   6883.5310    313.919     21.928      0.000      6249.559  7517.503\n",
        "X            546.1840     30.519     17.896      0.000       484.549   607.819\n",
        "==============================================================================\n",
        "Omnibus:                        2.293   Durbin-Watson:                   2.237\n",
        "Prob(Omnibus):                  0.318   Jarque-Bera (JB):                1.362\n",
        "Skew:                          -0.077   Prob(JB):                        0.506\n",
        "Kurtosis:                       2.171   Cond. No.                         33.5\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Dep. Variable*\n",
      "    the variable to be fitted; here, the \"Salary\"\n",
      "\n",
      "*Model*\n",
      "    OLS = ordinary least squares\n",
      "\n",
      "*Df Residuals*\n",
      "    The number of observations, minus the number of parameters fitted.\n",
      "\n",
      "*Df model*\n",
      "    \"Degree of Freedom\" of the model, i.e. the dimensionnality of the subspace spanned by the model. This entails that the intercept is not counted.\n",
      "\n",
      "*R-squared*\n",
      "    Coefficient of Determination = (S0-Sm)/S0\n",
      "\n",
      "*Adj. R-squared*\n",
      "    The adjusted R2 coefficient, which takes into consideration the number of model paramters.\n",
      "\n",
      "*F-statistic*, and corresponding *Prob*\n",
      "    F-test on the regression model, if it is significantly different from the minimum model (i.e. a constant offset only)\n",
      "\n",
      "*Log-Likelihood*\n",
      "    Maximum log-likelihood value for the model.\n",
      "\n",
      "*AIC*\n",
      "    Aiken's Information Criterion, for the assessment of the model.\n",
      "\n",
      "*BIC*\n",
      "    Bayesian Information Criterion, for the assessment of the model.\n",
      "\n",
      "The values in the lowest box describe properties of the residuals (\"Skew\", \"Kurtosisi\"), as well as tests on the residuals."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Inspect the Design Matrix, and demonstrate the Model Predictions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look at the design matrix created for us. Every results instance has a reference to the model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lm.model.exog[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "array([[ 1.,  0.,  0.,  1.,  1.],\n",
        "       [ 1.,  0.,  1.,  0.,  1.],\n",
        "       [ 1.,  0.,  1.,  1.,  1.],\n",
        "       [ 1.,  1.,  0.,  0.,  1.],\n",
        "       [ 1.,  0.,  1.,  0.,  1.],\n",
        "       [ 1.,  1.,  0.,  1.,  2.],\n",
        "       [ 1.,  1.,  0.,  0.,  2.],\n",
        "       [ 1.,  0.,  0.,  0.,  2.],\n",
        "       [ 1.,  0.,  1.,  0.,  2.],\n",
        "       [ 1.,  1.,  0.,  0.,  3.]])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we initially passed in a DataFrame, we have a transformed DataFrame available."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(lm.model.data.orig_exog.head(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   Intercept  C(E)[T.2]  C(E)[T.3]  C(M)[T.1]  X\n",
        "0          1          0          0          1  1\n",
        "1          1          0          1          0  1\n",
        "2          1          0          1          1  1\n",
        "3          1          1          0          0  1\n",
        "4          1          0          1          0  1\n",
        "5          1          1          0          1  2\n",
        "6          1          1          0          0  2\n",
        "7          1          0          0          0  2\n",
        "8          1          0          1          0  2\n",
        "9          1          1          0          0  3\n",
        "\n",
        "[10 rows x 5 columns]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a reference to the original untouched data in"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(lm.model.data.frame.head(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "       S  X  E  M\n",
        "0  13876  1  1  1\n",
        "1  11608  1  3  0\n",
        "2  18701  1  3  1\n",
        "3  11283  1  2  0\n",
        "4  11767  1  3  0\n",
        "5  20872  2  2  1\n",
        "6  11772  2  2  0\n",
        "7  10535  2  1  0\n",
        "8  12195  2  3  0\n",
        "9  12313  3  2  0\n",
        "\n",
        "[10 rows x 4 columns]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you use the formula interface, statsmodels remembers this transformation. Say you want to know the predicted salary for someone with 12 years experience and a Master's degree who is in a management position"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lm.predict({'X' : [12], 'M' : [1], 'E' : [2]})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "array([ 24617.37207242])"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Check the Residuals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we've assumed that the effect of experience is the same for each level of education and professional role.\n",
      "Perhaps this assumption isn't merited. We can formally test this using some interactions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can start by seeing if our model assumptions are met. Let's look at a residuals plot, with the groups separated."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "resid = lm.resid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(12,8))\n",
      "xticks = []\n",
      "ax = fig.add_subplot(111, xlabel='Group (E, M)', ylabel='Residuals')\n",
      "for values, group in factor_groups:\n",
      "    i,j = values\n",
      "    xticks.append(str((i, j)))\n",
      "    group_num = i*2 + j - 1 # for plotting purposes\n",
      "    x = [group_num] * len(group)\n",
      "    ax.scatter(x, resid[group.index], marker=symbols[j], color=colors[i-1],\n",
      "            s=144, edgecolors='black')\n",
      "ax.set_xticks([1,2,3,4,5,6])\n",
      "ax.set_xticklabels(xticks)\n",
      "ax.axis('tight');\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obviously, the linear model alone does not capture all model features, as the residuals are not normally distributed within each group. To improve the model, we check if interactions between the model parameters can explain the group differences."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Add an Interaction"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Interaction Salary*Experience"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add an interaction between salary and experience, allowing different intercepts for level of experience.\n",
      "\n",
      "$$S_i = \\beta_0+\\beta_1X_i+\\beta_2E_{i2}+\\beta_3E_{i3}+\\beta_4M_i+\\beta_5E_{i2}X_i+\\beta_6E_{i3}X_i+\\epsilon_i$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interX_lm = ols('S ~ C(E)*X + C(M)', salary_table).fit()\n",
      "print(interX_lm.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                      S   R-squared:                       0.961\n",
        "Model:                            OLS   Adj. R-squared:                  0.955\n",
        "Method:                 Least Squares   F-statistic:                     158.6\n",
        "Date:                Tue, 25 Feb 2014   Prob (F-statistic):           8.23e-26\n",
        "Time:                        15:10:10   Log-Likelihood:                -379.47\n",
        "No. Observations:                  46   AIC:                             772.9\n",
        "Df Residuals:                      39   BIC:                             785.7\n",
        "Df Model:                           6                                         \n",
        "===============================================================================\n",
        "                  coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "-------------------------------------------------------------------------------\n",
        "Intercept    7256.2800    549.494     13.205      0.000      6144.824  8367.736\n",
        "C(E)[T.2]    4172.5045    674.966      6.182      0.000      2807.256  5537.753\n",
        "C(E)[T.3]    3946.3649    686.693      5.747      0.000      2557.396  5335.333\n",
        "C(M)[T.1]    7102.4539    333.442     21.300      0.000      6428.005  7776.903\n",
        "X             632.2878     53.185     11.888      0.000       524.710   739.865\n",
        "C(E)[T.2]:X  -125.5147     69.863     -1.797      0.080      -266.826    15.796\n",
        "C(E)[T.3]:X  -141.2741     89.281     -1.582      0.122      -321.861    39.313\n",
        "==============================================================================\n",
        "Omnibus:                        0.432   Durbin-Watson:                   2.179\n",
        "Prob(Omnibus):                  0.806   Jarque-Bera (JB):                0.590\n",
        "Skew:                           0.144   Prob(JB):                        0.744\n",
        "Kurtosis:                       2.526   Cond. No.                         69.7\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The \"Factor\" Experience has the \"Treatments\" (or elements) \"1\", \"2\", and \"3\". Since the \"Treatment 1\"([T.1]) is taken as the reference, it is not listed here explicitly. This is called \"corner-point\" approach.\n",
      "Similarly, the \"Factor\" Management has the \"Treatments\" \"0\" and \"1\". With \"0\" taken as the reference, only the term C(M)[T.1] is listed in the model.\n",
      "The interactions are described by the terms \"C(E)[T.i]:X\"."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Test the Interaction Management*Experience"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test that $\\beta_5 = \\beta_6 = 0$. We can use anova_lm or we can use an F-test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(anova_lm(lm, interX_lm())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "unexpected EOF while parsing (<ipython-input-14-e69d2393f7e1>, line 1)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-e69d2393f7e1>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print(anova_lm(lm, interX_lm())\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(interX_lm.f_test('C(E)[T.2]:X = C(E)[T.3]:X = 0'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(interX_lm.f_test([[0,0,0,0,0,1,-1],[0,0,0,0,0,0,1]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Both tests show that the models are not significantly different. In other words, there is no interaction effect between Management and Experience in the data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The contrasts are created here under the hood by patsy."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall that F-tests are of the form $R\\beta = q$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "LC = interX_lm.model.data.orig_exog.design_info.linear_constraint('C(E)[T.2]:X = C(E)[T.3]:X = 0')\n",
      "print(LC.coefs)\n",
      "print(LC.constants)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Interact education with management"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interM_lm = ols('S ~ X + C(E)*C(M)', salary_table).fit()\n",
      "print(interM_lm.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                      S   R-squared:                       0.999\n",
        "Model:                            OLS   Adj. R-squared:                  0.999\n",
        "Method:                 Least Squares   F-statistic:                     5517.\n",
        "Date:                Tue, 25 Feb 2014   Prob (F-statistic):           1.67e-55\n",
        "Time:                        15:10:10   Log-Likelihood:                -298.74\n",
        "No. Observations:                  46   AIC:                             611.5\n",
        "Df Residuals:                      39   BIC:                             624.3\n",
        "Df Model:                           6                                         \n",
        "=======================================================================================\n",
        "                          coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "---------------------------------------------------------------------------------------\n",
        "Intercept            9472.6854     80.344    117.902      0.000      9310.175  9635.196\n",
        "C(E)[T.2]            1381.6706     77.319     17.870      0.000      1225.279  1538.063\n",
        "C(E)[T.3]            1730.7483    105.334     16.431      0.000      1517.690  1943.806\n",
        "C(M)[T.1]            3981.3769    101.175     39.351      0.000      3776.732  4186.022\n",
        "C(E)[T.2]:C(M)[T.1]  4902.5231    131.359     37.322      0.000      4636.825  5168.222\n",
        "C(E)[T.3]:C(M)[T.1]  3066.0351    149.330     20.532      0.000      2763.986  3368.084\n",
        "X                     496.9870      5.566     89.283      0.000       485.728   508.246\n",
        "==============================================================================\n",
        "Omnibus:                       74.761   Durbin-Watson:                   2.244\n",
        "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1037.873\n",
        "Skew:                          -4.103   Prob(JB):                    4.25e-226\n",
        "Kurtosis:                      24.776   Cond. No.                         79.0\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(anova_lm(lm, interM_lm))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   df_resid              ssr  df_diff          ss_diff           F  \\\n",
        "0        41  43280719.492876        0              NaN         NaN   \n",
        "1        39   1178167.864864        2  42102551.628012  696.844466   \n",
        "\n",
        "         Pr(>F)  \n",
        "0           NaN  \n",
        "1  3.025504e-31  \n",
        "\n",
        "[2 rows x 6 columns]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The interaction effect Management*Education is highly significant!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "infl = interM_lm.get_influence()\n",
      "resid = infl.resid_studentized_internal"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Check the residuals of the extended model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(12,8))\n",
      "ax = fig.add_subplot(111, xlabel='X', ylabel='standardized resids')\n",
      "\n",
      "for values, group in factor_groups:\n",
      "    i,j = values\n",
      "    idx = group.index\n",
      "    ax.scatter(X[idx], resid[idx], marker=symbols[j], color=colors[i-1],\n",
      "            s=144, edgecolors='black')\n",
      "ax.axis('tight');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There looks to be an outlier."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Find and Remove the Outlier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outl = interM_lm.outlier_test('fdr_bh')\n",
      "outl.sort('unadj_p', inplace=True)\n",
      "print(outl)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    student_resid       unadj_p     fdr_bh(p)\n",
        "32     -14.950832  1.676948e-17  7.713960e-16\n",
        "33       1.288001  2.055339e-01  9.599254e-01\n",
        "23       1.023194  3.126860e-01  9.599254e-01\n",
        "41       0.942329  3.519767e-01  9.599254e-01\n",
        "11       0.896574  3.755915e-01  9.599254e-01\n",
        "5        0.890643  3.787254e-01  9.599254e-01\n",
        "39       0.835081  4.088920e-01  9.599254e-01\n",
        "38       0.819164  4.178011e-01  9.599254e-01\n",
        "21       0.729762  4.700104e-01  9.599254e-01\n",
        "20      -0.726132  4.722071e-01  9.599254e-01\n",
        "30       0.704477  4.854309e-01  9.599254e-01\n",
        "28       0.603122  5.500109e-01  9.599254e-01\n",
        "44      -0.599720  5.522526e-01  9.599254e-01\n",
        "1       -0.590487  5.583601e-01  9.599254e-01\n",
        "17      -0.564832  5.755078e-01  9.599254e-01\n",
        "0       -0.482474  6.322368e-01  9.599254e-01\n",
        "34      -0.474793  6.376517e-01  9.599254e-01\n",
        "6       -0.464031  6.452728e-01  9.599254e-01\n",
        "7        0.428795  6.704934e-01  9.599254e-01\n",
        "45      -0.426443  6.721915e-01  9.599254e-01\n",
        "4        0.424446  6.736338e-01  9.599254e-01\n",
        "3       -0.418531  6.779148e-01  9.599254e-01\n",
        "35       0.382327  7.043486e-01  9.599254e-01\n",
        "43       0.360696  7.203240e-01  9.599254e-01\n",
        "12       0.357944  7.223662e-01  9.599254e-01\n",
        "37       0.342548  7.338259e-01  9.599254e-01\n",
        "2       -0.291688  7.721113e-01  9.599254e-01\n",
        "24       0.287114  7.755852e-01  9.599254e-01\n",
        "31      -0.285085  7.771273e-01  9.599254e-01\n",
        "36      -0.275068  7.847542e-01  9.599254e-01\n",
        "13      -0.268726  7.895942e-01  9.599254e-01\n",
        "27      -0.259790  7.964281e-01  9.599254e-01\n",
        "15       0.252085  8.023339e-01  9.599254e-01\n",
        "16       0.249972  8.039551e-01  9.599254e-01\n",
        "42       0.195876  8.457513e-01  9.599254e-01\n",
        "9       -0.194723  8.466475e-01  9.599254e-01\n",
        "10       0.190826  8.496777e-01  9.599254e-01\n",
        "26      -0.165977  8.690547e-01  9.599254e-01\n",
        "19       0.165168  8.696870e-01  9.599254e-01\n",
        "25      -0.161711  8.723902e-01  9.599254e-01\n",
        "14       0.147997  8.831273e-01  9.599254e-01\n",
        "29       0.147288  8.836831e-01  9.599254e-01\n",
        "40      -0.129912  8.973215e-01  9.599254e-01\n",
        "18      -0.072460  9.426159e-01  9.854621e-01\n",
        "22       0.016188  9.871691e-01  9.878792e-01\n",
        "8       -0.015292  9.878792e-01  9.878792e-01\n",
        "\n",
        "[46 rows x 3 columns]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "idx = salary_table.index.drop(32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(idx)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], dtype='int64')\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Rerun the original linear model without the outlier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lm32 = ols('S ~ C(E) + X + C(M)', data=salary_table, subset=idx).fit()\n",
      "print(lm32.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                      S   R-squared:                       0.955\n",
        "Model:                            OLS   Adj. R-squared:                  0.950\n",
        "Method:                 Least Squares   F-statistic:                     211.7\n",
        "Date:                Tue, 25 Feb 2014   Prob (F-statistic):           2.45e-26\n",
        "Time:                        15:10:12   Log-Likelihood:                -373.79\n",
        "No. Observations:                  45   AIC:                             757.6\n",
        "Df Residuals:                      40   BIC:                             766.6\n",
        "Df Model:                           4                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept   8044.7518    392.781     20.482      0.000      7250.911  8838.592\n",
        "C(E)[T.2]   3129.5286    370.470      8.447      0.000      2380.780  3878.277\n",
        "C(E)[T.3]   2999.4451    416.712      7.198      0.000      2157.238  3841.652\n",
        "C(M)[T.1]   6866.9856    323.991     21.195      0.000      6212.175  7521.796\n",
        "X            545.7855     30.912     17.656      0.000       483.311   608.260\n",
        "==============================================================================\n",
        "Omnibus:                        2.511   Durbin-Watson:                   2.265\n",
        "Prob(Omnibus):                  0.285   Jarque-Bera (JB):                1.400\n",
        "Skew:                          -0.044   Prob(JB):                        0.496\n",
        "Kurtosis:                       2.140   Cond. No.                         33.1\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Interaction Education*Experience"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interX_lm32 = ols('S ~ C(E) * X + C(M)', data=salary_table, subset=idx).fit()\n",
      "print(interX_lm32.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                      S   R-squared:                       0.959\n",
        "Model:                            OLS   Adj. R-squared:                  0.952\n",
        "Method:                 Least Squares   F-statistic:                     147.7\n",
        "Date:                Tue, 25 Feb 2014   Prob (F-statistic):           8.97e-25\n",
        "Time:                        15:10:12   Log-Likelihood:                -371.70\n",
        "No. Observations:                  45   AIC:                             757.4\n",
        "Df Residuals:                      38   BIC:                             770.0\n",
        "Df Model:                           6                                         \n",
        "===============================================================================\n",
        "                  coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "-------------------------------------------------------------------------------\n",
        "Intercept    7266.0887    558.872     13.001      0.000      6134.711  8397.466\n",
        "C(E)[T.2]    4162.0846    685.728      6.070      0.000      2773.900  5550.269\n",
        "C(E)[T.3]    3940.4359    696.067      5.661      0.000      2531.322  5349.549\n",
        "C(M)[T.1]    7088.6387    345.587     20.512      0.000      6389.035  7788.243\n",
        "X             631.6892     53.950     11.709      0.000       522.473   740.905\n",
        "C(E)[T.2]:X  -125.5009     70.744     -1.774      0.084      -268.714    17.712\n",
        "C(E)[T.3]:X  -139.8410     90.728     -1.541      0.132      -323.511    43.829\n",
        "==============================================================================\n",
        "Omnibus:                        0.617   Durbin-Watson:                   2.194\n",
        "Prob(Omnibus):                  0.734   Jarque-Bera (JB):                0.728\n",
        "Skew:                           0.162   Prob(JB):                        0.695\n",
        "Kurtosis:                       2.468   Cond. No.                         68.7\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table3 = anova_lm(lm32, interX_lm32)\n",
      "print(table3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   df_resid              ssr  df_diff         ss_diff         F    Pr(>F)\n",
        "0        40  43209096.482552        0             NaN       NaN       NaN\n",
        "1        38  39374237.269069        2  3834859.213482  1.850508  0.171042\n",
        "\n",
        "[2 rows x 6 columns]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, this is not significant, and can be left away."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Final Result"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Result from the Interaction Test: the significant model with Interaction Experience*Management, without the outlier #32"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interM_lm32 = ols('S ~ X + C(E) * C(M)', data=salary_table, subset=idx).fit()\n",
      "print(anova_lm(lm32, interM_lm32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   df_resid              ssr  df_diff          ss_diff            F  \\\n",
        "0        40  43209096.482552        0              NaN          NaN   \n",
        "1        38    171188.119937        2  43037908.362615  4776.734853   \n",
        "\n",
        "         Pr(>F)  \n",
        "0           NaN  \n",
        "1  2.291239e-46  \n",
        "\n",
        "[2 rows x 6 columns]\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Re-plotting the residuals"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "resid = interM_lm32.get_influence().summary_frame()['standard_resid']\n",
      "fig = plt.figure(figsize=(12,8))\n",
      "ax = fig.add_subplot(111, xlabel='X[~[32]]', ylabel='standardized resids')\n",
      "\n",
      "for values, group in factor_groups:\n",
      "    i,j = values\n",
      "    idx = group.index\n",
      "    ax.scatter(X[idx], resid[idx], marker=symbols[j], color=colors[i-1],\n",
      "            s=144, edgecolors='black')\n",
      "ax.axis('tight');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A final plot of the fitted values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lm_final = ols('S ~ X + C(E)*C(M)', data=salary_table.drop([32])).fit()\n",
      "mf = lm_final.model.data.orig_exog\n",
      "lstyle = ['-','--']\n",
      "\n",
      "fig = plt.figure(figsize=(12,8))\n",
      "ax = fig.add_subplot(111, xlabel='Experience', ylabel='Salary')\n",
      "\n",
      "for values, group in factor_groups:\n",
      "    i,j = values\n",
      "    idx = group.index\n",
      "    ax.scatter(X[idx], S[idx], marker=symbols[j], color=colors[i-1],\n",
      "            s=144, edgecolors='black')\n",
      "    # drop NA because there is no idx 32 in the final model\n",
      "    ax.plot(mf.X[idx].dropna(), lm_final.fittedvalues[idx].dropna(),\n",
      "            ls=lstyle[j], color=colors[i-1])\n",
      "ax.axis('tight');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Interaction Plot Salary | Experience"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From our first look at the data, the difference between Master's and PhD in the management group is different than in the non-management group. This is an interaction between the two qualitative variables management, M and education, E. We can visualize this by first removing the effect of experience, then plotting the means within each of the 6 groups using interaction.plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "U = S - X * interX_lm32.params['X']\n",
      "U.name = 'Salary|X'\n",
      "\n",
      "fig = plt.figure(figsize=(12,8))\n",
      "ax = fig.add_subplot(111)\n",
      "ax = interaction_plot(E, M, U, colors=['red','blue'], markers=['^','D'],\n",
      "        markersize=10, ax=ax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Example 2: Minority Employment Data - ABLine plotting"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "TEST  - Job Aptitude Test Score\n",
      "ETHN  - 1 if minority, 0 otherwise\n",
      "JPERF - Job performance evaluation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    minority_table = pandas.read_table('minority.table')\n",
      "except: # don't have data already\n",
      "    url = 'http://stats191.stanford.edu/data/minority.table'\n",
      "    minority_table = pandas.read_table(url)\n",
      "    minority_table.to_csv('minority.table', sep=\"\\t\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "factor_group = minority_table.groupby(['ETHN'])\n",
      "\n",
      "fig = plt.figure(figsize=(12,8))\n",
      "ax = fig.add_subplot(111, xlabel='TEST', ylabel='JPERF')\n",
      "colors = ['purple', 'green']\n",
      "markers = ['o', 'v']\n",
      "for factor, group in factor_group:\n",
      "    ax.scatter(group['TEST'], group['JPERF'], color=colors[factor],\n",
      "                marker=markers[factor], s=12**2)\n",
      "ax.legend(['ETHN == 1', 'ETHN == 0'], scatterpoints=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "<matplotlib.legend.Legend at 0xde9c518>"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_lm = ols('JPERF ~ TEST', data=minority_table).fit()\n",
      "print(min_lm.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                  JPERF   R-squared:                       0.517\n",
        "Model:                            OLS   Adj. R-squared:                  0.490\n",
        "Method:                 Least Squares   F-statistic:                     19.25\n",
        "Date:                Tue, 25 Feb 2014   Prob (F-statistic):           0.000356\n",
        "Time:                        15:10:14   Log-Likelihood:                -36.614\n",
        "No. Observations:                  20   AIC:                             77.23\n",
        "Df Residuals:                      18   BIC:                             79.22\n",
        "Df Model:                           1                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept      1.0350      0.868      1.192      0.249        -0.789     2.859\n",
        "TEST           2.3605      0.538      4.387      0.000         1.230     3.491\n",
        "==============================================================================\n",
        "Omnibus:                        0.324   Durbin-Watson:                   2.896\n",
        "Prob(Omnibus):                  0.850   Jarque-Bera (JB):                0.483\n",
        "Skew:                          -0.186   Prob(JB):                        0.785\n",
        "Kurtosis:                       2.336   Cond. No.                         5.26\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(12,8))\n",
      "ax = fig.add_subplot(111, xlabel='TEST', ylabel='JPERF')\n",
      "for factor, group in factor_group:\n",
      "    ax.scatter(group['TEST'], group['JPERF'], color=colors[factor],\n",
      "                marker=markers[factor], s=12**2)\n",
      "ax.legend(['ETHN == 1', 'ETHN == 0'], scatterpoints=1, loc='upper left')\n",
      "fig = abline_plot(model_results = min_lm, ax=ax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_lm2 = ols('JPERF ~ TEST + TEST:ETHN', data=minority_table).fit()\n",
      "print(min_lm2.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                  JPERF   R-squared:                       0.632\n",
        "Model:                            OLS   Adj. R-squared:                  0.589\n",
        "Method:                 Least Squares   F-statistic:                     14.59\n",
        "Date:                Tue, 25 Feb 2014   Prob (F-statistic):           0.000204\n",
        "Time:                        15:10:15   Log-Likelihood:                -33.891\n",
        "No. Observations:                  20   AIC:                             73.78\n",
        "Df Residuals:                      17   BIC:                             76.77\n",
        "Df Model:                           2                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept      1.1211      0.780      1.437      0.169        -0.525     2.768\n",
        "TEST           1.8276      0.536      3.412      0.003         0.698     2.958\n",
        "TEST:ETHN      0.9161      0.397      2.306      0.034         0.078     1.754\n",
        "==============================================================================\n",
        "Omnibus:                        0.388   Durbin-Watson:                   3.008\n",
        "Prob(Omnibus):                  0.823   Jarque-Bera (JB):                0.514\n",
        "Skew:                           0.050   Prob(JB):                        0.773\n",
        "Kurtosis:                       2.221   Cond. No.                         5.96\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(12,8))\n",
      "ax = fig.add_subplot(111, xlabel='TEST', ylabel='JPERF')\n",
      "for factor, group in factor_group:\n",
      "    ax.scatter(group['TEST'], group['JPERF'], color=colors[factor],\n",
      "                marker=markers[factor], s=12**2)\n",
      "\n",
      "fig = abline_plot(intercept = min_lm2.params['Intercept'],\n",
      "                 slope = min_lm2.params['TEST'], ax=ax, color='purple')\n",
      "ax = fig.axes[0]\n",
      "fig = abline_plot(intercept = min_lm2.params['Intercept'],\n",
      "        slope = min_lm2.params['TEST'] + min_lm2.params['TEST:ETHN'],\n",
      "        ax=ax, color='green')\n",
      "ax.legend(['ETHN == 1', 'ETHN == 0'], scatterpoints=1, loc='upper left');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_lm3 = ols('JPERF ~ TEST + ETHN', data=minority_table).fit()\n",
      "print(min_lm3.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                  JPERF   R-squared:                       0.572\n",
        "Model:                            OLS   Adj. R-squared:                  0.522\n",
        "Method:                 Least Squares   F-statistic:                     11.38\n",
        "Date:                Tue, 25 Feb 2014   Prob (F-statistic):           0.000731\n",
        "Time:                        15:10:15   Log-Likelihood:                -35.390\n",
        "No. Observations:                  20   AIC:                             76.78\n",
        "Df Residuals:                      17   BIC:                             79.77\n",
        "Df Model:                           2                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept      0.6120      0.887      0.690      0.500        -1.260     2.483\n",
        "TEST           2.2988      0.522      4.400      0.000         1.197     3.401\n",
        "ETHN           1.0276      0.691      1.487      0.155        -0.430     2.485\n",
        "==============================================================================\n",
        "Omnibus:                        0.251   Durbin-Watson:                   3.028\n",
        "Prob(Omnibus):                  0.882   Jarque-Bera (JB):                0.437\n",
        "Skew:                          -0.059   Prob(JB):                        0.804\n",
        "Kurtosis:                       2.286   Cond. No.                         5.72\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(12,8))\n",
      "ax = fig.add_subplot(111, xlabel='TEST', ylabel='JPERF')\n",
      "for factor, group in factor_group:\n",
      "    ax.scatter(group['TEST'], group['JPERF'], color=colors[factor],\n",
      "                marker=markers[factor], s=12**2)\n",
      "\n",
      "fig = abline_plot(intercept = min_lm3.params['Intercept'],\n",
      "                 slope = min_lm3.params['TEST'], ax=ax, color='purple')\n",
      "\n",
      "ax = fig.axes[0]\n",
      "fig = abline_plot(intercept = min_lm3.params['Intercept'] + min_lm3.params['ETHN'],\n",
      "        slope = min_lm3.params['TEST'], ax=ax, color='green')\n",
      "ax.legend(['ETHN == 1', 'ETHN == 0'], scatterpoints=1, loc='upper left');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_lm4 = ols('JPERF ~ TEST * ETHN', data=minority_table).fit()\n",
      "print(min_lm4.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                  JPERF   R-squared:                       0.664\n",
        "Model:                            OLS   Adj. R-squared:                  0.601\n",
        "Method:                 Least Squares   F-statistic:                     10.55\n",
        "Date:                Tue, 25 Feb 2014   Prob (F-statistic):           0.000451\n",
        "Time:                        15:10:15   Log-Likelihood:                -32.971\n",
        "No. Observations:                  20   AIC:                             73.94\n",
        "Df Residuals:                      16   BIC:                             77.92\n",
        "Df Model:                           3                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept      2.0103      1.050      1.914      0.074        -0.216     4.236\n",
        "TEST           1.3134      0.670      1.959      0.068        -0.108     2.735\n",
        "ETHN          -1.9132      1.540     -1.242      0.232        -5.179     1.352\n",
        "TEST:ETHN      1.9975      0.954      2.093      0.053        -0.026     4.021\n",
        "==============================================================================\n",
        "Omnibus:                        3.377   Durbin-Watson:                   3.015\n",
        "Prob(Omnibus):                  0.185   Jarque-Bera (JB):                1.330\n",
        "Skew:                           0.120   Prob(JB):                        0.514\n",
        "Kurtosis:                       1.760   Cond. No.                         13.8\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(12,8))\n",
      "ax = fig.add_subplot(111, ylabel='JPERF', xlabel='TEST')\n",
      "for factor, group in factor_group:\n",
      "    ax.scatter(group['TEST'], group['JPERF'], color=colors[factor],\n",
      "                marker=markers[factor], s=12**2)\n",
      "\n",
      "fig = abline_plot(intercept = min_lm4.params['Intercept'],\n",
      "                 slope = min_lm4.params['TEST'], ax=ax, color='purple')\n",
      "ax = fig.axes[0]\n",
      "fig = abline_plot(intercept = min_lm4.params['Intercept'] + min_lm4.params['ETHN'],\n",
      "        slope = min_lm4.params['TEST'] + min_lm4.params['TEST:ETHN'],\n",
      "        ax=ax, color='green')\n",
      "ax.legend(['ETHN == 1', 'ETHN == 0'], scatterpoints=1, loc='upper left');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is there any effect of ETHN on slope or intercept?\n",
      "<br />\n",
      "Y ~ TEST vs. Y ~ TEST + ETHN + ETHN:TEST"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table5 = anova_lm(min_lm, min_lm4)\n",
      "print(table5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   df_resid        ssr  df_diff    ss_diff         F    Pr(>F)\n",
        "0        18  45.568297        0        NaN       NaN       NaN\n",
        "1        16  31.655473        2  13.912824  3.516061  0.054236\n",
        "\n",
        "[2 rows x 6 columns]\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is there any effect of ETHN on intercept?\n",
      "<br />\n",
      "Y ~ TEST vs. Y ~ TEST + ETHN"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table6 = anova_lm(min_lm, min_lm3)\n",
      "print(table6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   df_resid        ssr  df_diff   ss_diff         F    Pr(>F)\n",
        "0        18  45.568297        0       NaN       NaN       NaN\n",
        "1        17  40.321546        1  5.246751  2.212087  0.155246\n",
        "\n",
        "[2 rows x 6 columns]\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is there any effect of ETHN on slope?\n",
      "<br />\n",
      "Y ~ TEST vs. Y ~ TEST + ETHN:TEST"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table7 = anova_lm(min_lm, min_lm2)\n",
      "print(table7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   df_resid        ssr  df_diff    ss_diff         F    Pr(>F)\n",
        "0        18  45.568297        0        NaN       NaN       NaN\n",
        "1        17  34.707653        1  10.860644  5.319603  0.033949\n",
        "\n",
        "[2 rows x 6 columns]\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is it just the slope or both?\n",
      "<br />\n",
      "Y ~ TEST + ETHN:TEST vs Y ~ TEST + ETHN + ETHN:TEST"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "table8 = anova_lm(min_lm2, min_lm4)\n",
      "print(table8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   df_resid        ssr  df_diff  ss_diff         F    Pr(>F)\n",
        "0        17  34.707653        0      NaN       NaN       NaN\n",
        "1        16  31.655473        1  3.05218  1.542699  0.232115\n",
        "\n",
        "[2 rows x 6 columns]\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Two Way ANOVA - Kidney failure data"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Weight - (1,2,3) - Level of weight gan between treatments\n",
      "Duration - (1,2) - Level of duration of treatment\n",
      "Days - Time of stay in hospital"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    kidney_table = pandas.read_table('kidney.table')\n",
      "except:\n",
      "    url = 'http://stats191.stanford.edu/data/kidney.table'\n",
      "    kidney_table = pandas.read_table(url, delimiter=\" *\")\n",
      "    kidney_table.to_csv(\"kidney.table\", sep=\"\\t\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Explore the dataset, it's a balanced design\n",
      "print(kidney_table.groupby(['Weight', 'Duration']).size())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Weight  Duration\n",
        "1       1           10\n",
        "        2           10\n",
        "2       1           10\n",
        "        2           10\n",
        "3       1           10\n",
        "        2           10\n",
        "dtype: int64\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kt = kidney_table\n",
      "fig = plt.figure(figsize=(10,8))\n",
      "ax = fig.add_subplot(111)\n",
      "fig = interaction_plot(kt['Weight'], kt['Duration'], np.log(kt['Days']+1),\n",
      "        colors=['red', 'blue'], markers=['D','^'], ms=10, ax=ax)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\left(\\alpha\\beta\\right)_{ij}+\\epsilon_{ijk}$$\n",
      "\n",
      "with \n",
      "\n",
      "$$\\epsilon_{ijk}\\sim N\\left(0,\\sigma^2\\right)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(anova_lm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function anova_lm in module statsmodels.stats.anova:\n",
        "\n",
        "anova_lm(*args, **kwargs)\n",
        "    ANOVA table for one or more fitted linear models.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    args : fitted linear model results instance\n",
        "        One or more fitted linear models\n",
        "    scale : float\n",
        "        Estimate of variance, If None, will be estimated from the largest\n",
        "        model. Default is None.\n",
        "    test : str {\"F\", \"Chisq\", \"Cp\"} or None\n",
        "        Test statistics to provide. Default is \"F\".\n",
        "    typ : str or int {\"I\",\"II\",\"III\"} or {1,2,3}\n",
        "        The type of ANOVA test to perform. See notes.\n",
        "    robust : {None, \"hc0\", \"hc1\", \"hc2\", \"hc3\"}\n",
        "        Use heteroscedasticity-corrected coefficient covariance matrix.\n",
        "        If robust covariance is desired, it is recommended to use `hc3`.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    anova : DataFrame\n",
        "    A DataFrame containing.\n",
        "    \n",
        "    Notes\n",
        "    -----\n",
        "    Model statistics are given in the order of args. Models must have\n",
        "    been fit using the formula api.\n",
        "    \n",
        "    See Also\n",
        "    --------\n",
        "    model_results.compare_f_test, model_results.compare_lm_test\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> import statsmodels.api as sm\n",
        "    >>> from statsmodels.formula.api import ols\n",
        "    \n",
        "    >>> moore = sm.datasets.get_rdataset(\"Moore\", \"car\",\n",
        "    ...                                  cache=True) # load data\n",
        "    >>> data = moore.data\n",
        "    >>> data = data.rename(columns={\"partner.status\" :\n",
        "    ...                             \"partner_status\"}) # make name pythonic\n",
        "    >>> moore_lm = ols('conformity ~ C(fcategory, Sum)*C(partner_status, Sum)',\n",
        "    ...                 data=data).fit()\n",
        "    \n",
        "    >>> table = sm.stats.anova_lm(moore_lm, typ=2) # Type 2 ANOVA DataFrame\n",
        "    >>> print(table)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Things available in the calling namespace are available in the formula evaluation namespace"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kidney_lm = ols('np.log(Days+1) ~ C(Duration) * C(Weight)', data=kt).fit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ANOVA Type-I Sum of Squares\n",
      "<br /><br />\n",
      "SS(A) for factor A. <br />\n",
      "SS(B|A) for factor B. <br />\n",
      "SS(AB|B, A) for interaction AB. <br />"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(anova_lm(kidney_lm))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                       df     sum_sq   mean_sq          F    PR(>F)\n",
        "C(Duration)             1   2.339693  2.339693   4.358293  0.041562\n",
        "C(Weight)               2  16.971291  8.485645  15.806745  0.000004\n",
        "C(Duration):C(Weight)   2   0.635658  0.317829   0.592040  0.556748\n",
        "Residual               54  28.989198  0.536837        NaN       NaN\n",
        "\n",
        "[4 rows x 5 columns]\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ANOVA Type-II Sum of Squares\n",
      "<br /><br />\n",
      "SS(A|B) for factor A. <br />\n",
      "SS(B|A) for factor B. <br />"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(anova_lm(kidney_lm, typ=2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "c:\\WinPython-64bit-3.3.3.3\\python-3.3.3.amd64\\lib\\site-packages\\statsmodels\\stats\\anova.py:211: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_index,col_indexer] = value instead\n",
        "  table.ix[i][test] = test_value = f.fvalue\n",
        "c:\\WinPython-64bit-3.3.3.3\\python-3.3.3.amd64\\lib\\site-packages\\statsmodels\\stats\\anova.py:212: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_index,col_indexer] = value instead\n",
        "  table.ix[i][pr_test] = f.pvalue\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                          sum_sq  df          F    PR(>F)\n",
        "C(Duration)             2.339693   1   4.358293  0.041562\n",
        "C(Weight)              16.971291   2  15.806745  0.000004\n",
        "C(Duration):C(Weight)   0.635658   2   0.592040  0.556748\n",
        "Residual                0.000000   0   0.000000  0.000000\n",
        "\n",
        "[4 rows x 4 columns]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "c:\\WinPython-64bit-3.3.3.3\\python-3.3.3.amd64\\lib\\site-packages\\statsmodels\\stats\\anova.py:215: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_index,col_indexer] = value instead\n",
        "  table.ix[i]['df'] = r\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "ANOVA Type-III Sum of Squares\n",
      "<br /><br />\n",
      "SS(A|B, AB) for factor A. <br />\n",
      "SS(B|A, AB) for factor B. <br />"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(anova_lm(ols('np.log(Days+1) ~ C(Duration, Sum) * C(Weight, Poly)', \n",
      "                   data=kt).fit(), typ=3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "c:\\WinPython-64bit-3.3.3.3\\python-3.3.3.amd64\\lib\\site-packages\\statsmodels\\stats\\anova.py:250: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_index,col_indexer] = value instead\n",
        "  table.ix[i][test] = test_value = f.fvalue\n",
        "c:\\WinPython-64bit-3.3.3.3\\python-3.3.3.amd64\\lib\\site-packages\\statsmodels\\stats\\anova.py:251: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_index,col_indexer] = value instead\n",
        "  table.ix[i][pr_test] = f.pvalue\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                      sum_sq  df           F        PR(>F)\n",
        "Intercept                         156.301830   1  291.153237  2.077589e-23\n",
        "C(Duration, Sum)                    2.339693   1    4.358293  4.156170e-02\n",
        "C(Weight, Poly)                    16.971291   2   15.806745  3.944502e-06\n",
        "C(Duration, Sum):C(Weight, Poly)    0.635658   2    0.592040  5.567479e-01\n",
        "Residual                            0.000000   0    0.000000  0.000000e+00\n",
        "\n",
        "[5 rows x 4 columns]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "c:\\WinPython-64bit-3.3.3.3\\python-3.3.3.amd64\\lib\\site-packages\\statsmodels\\stats\\anova.py:254: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\n",
        "Try using .loc[row_index,col_indexer] = value instead\n",
        "  table.ix[i]['df'] = r\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Excercise: Find the 'best' model for the kidney failure dataset"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}