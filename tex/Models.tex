\chapter{ Statistical Models } \index{general}{statistical modeling}

\section{Model language}
The mini-language commonly used now in statistics to describe formulas was first used in the languages $R$ and $S$, but is now also available in Python through the module \emph{patsy}.

For instance, if we have some variable $y$, and we want to regress it against some other variables $x, a, b$, and the interaction of a and b, then we simply write

\begin{equation}
    y \sim x + a + b + a:b
\end{equation}

The symbols in Table \ref{tab:syntax} are used on the right hand side to denote different interactions.

\begin{table}
  \centering
  \footnotesize{
  \begin{tabular}{ p{2cm} p{9cm} }
     Operator & Meaning \\
     \hline
    $\sim $ &	Separate the left-hand side from the right-hand side. If omitted, formula is assumed right-hand side only. \\
    + &	Combines terms on either side (set union). \\
    - &	Removes terms on the right from set of terms on the left (set difference). \\
    * &	a*b is shorthand for the expansion a + b + a:b. \\
    / &	a/b is shorthand for the expansion a + a:b. It is used when b is nested within a (e.g., states and counties) \\
    : &	Computes the interaction between terms on the left and right. \\
    ** & Takes a set of terms on the left and an integer n on the right and computes the * of that set of terms with itself n times.\\
     \hline
  \end{tabular}
  }
  \caption{Formula sytax}
\end{table}\label{tab:syntax}

A complete set of the description is found under \cite{patsy}

\subsection{Design Matrix}

\subsubsection{Definition}
A very general definition of a regression model is the following:

\begin{equation}
  y =f(x,\epsilon)
\end{equation}
In the case of a linear regression model, the function f is simply the affine function, and the model can be rewritten as:
\begin{equation}
    y=X\beta+ \epsilon,
\end{equation}
$Y$ is a vector of dimension $(n \times 1)$ and is called the \emph{endogenous variable}, the \emph{Design matrix} $X$ is a matrix of dimension $(n \times k)$ where each column is an explanatory variable, and $\epsilon$ is the error term. $\beta$ is the vector of dimension $(k \times 1)$ and contains the parameters we want to estimate.

\subsubsection{Examples}
\paragraph{Simple Regression}
Example of \emph{simple linear regression} with 7 observations.
Suppose there are 7 data points $\left\{ {{y_i},{x_i}} \right\}$, where $i=1,2,…,7$. The simple linear regression model is

\begin{equation}
  y_i = \beta_0 + \beta_1 x_i +\epsilon_i, \,
\end{equation}

where $\beta_0$ is the y-intercept and $\beta_1$ is the slope of the regression line. This model can be represented in matrix form as

\begin{equation}
  \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ y_6 \\ y_7 \end{bmatrix}
  =
  \begin{bmatrix}1 & x_1  \\1 & x_2  \\1 & x_3  \\1 & x_4  \\1 & x_5  \\1 & x_6 \\ 1 & x_7  \end{bmatrix}
  \begin{bmatrix} \beta_0 \\ \beta_1  \end{bmatrix}
  +
  \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 \\ \epsilon_6 \\ \epsilon_7 \end{bmatrix}
\end{equation}
where the first column of ones in the design matrix represents the y-intercept term while the second column is the x-values associated with the y-value.

\paragraph{Multiple Regression} \index{general}{regression!multiple}
Example of \emph{multiple regression} with covariates (i.e. independent variables) $w_i$ and $x_i$.
Again suppose that the data are 7 observations, and for each observed value to be predicted ($y_i$), there are two covariates that were also observed $w_i$ and $x_i$. The model to be considered is
\begin{equation}
  y_i = \beta_0 + \beta_1 w_i + \beta_2 x_i + \epsilon_i
\end{equation}
This model can be written in matrix terms as

\begin{equation}
  \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ y_6 \\ y_7 \end{bmatrix} =
    \begin{bmatrix} 1 & w_1 & x_1  \\1 & w_2 & x_2  \\1 & w_3 & x_3  \\1 & w_4 & x_4  \\1 & w_5 & x_5  \\1 & w_6 & x_6 \\ 1& w_7  & x_7  \end{bmatrix}
    \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2  \end{bmatrix}
    +
    \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 \\ \epsilon_6 \\ \epsilon_7 \end{bmatrix}
\end{equation}

\paragraph{One-way ANOVA (Cell Means Model)}
Example with a one-way analysis of variance (ANOVA) with 3 groups and 7 observations. The given data set has the first three observations belonging to the first group, the following two observations belong to the second group and the final two observations are from the third group.
If the model to be fit is just the mean of each group, then the model is

\begin{equation}
  y_{ij} = \mu_i + \epsilon_{ij}
\end{equation}

which can be written

\begin{equation}
  \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ y_6 \\ y_7 \end{bmatrix} =
  \begin{bmatrix}1 & 0 & 0 \\1 &0  &0 \\ 1 & 0 & 0 \\  0 & 1 & 0 \\  0 & 1 & 0 \\  0 & 0 & 1 \\  0 & 0 & 1\end{bmatrix}
  \begin{bmatrix}\mu_1 \\ \mu_2 \\ \mu_3  \end{bmatrix}
  +
  \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 \\ \epsilon_6 \\ \epsilon_7 \end{bmatrix}
\end{equation}
It should be emphasized that in this model $\mu_i$ represents the mean of the $i$th group.

\paragraph{One-way ANOVA (offset from reference group)}
The ANOVA model could be equivalently written as each group parameter $\tau_i$ being an offset from some overall reference.  Typically this reference point is taken to be one of the groups under consideration. This makes sense in the context of comparing multiple treatment groups to a control group and the control group is considered the "reference". In this example, group 1 was chosen to be the reference group. As such the model to be fit is:
\begin{equation}
  y_{ij} = \mu + \tau_i + \epsilon_{ij}
\end{equation}
with the constraint that $\tau_1$ is zero.

\begin{equation}
  \begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ y_6 \\ y_7 \end{bmatrix} =
  \begin{bmatrix}1 &0 &0 \\1 &0  &0 \\ 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 0 \\ 1 & 0 & 1 \\ 1  & 0 & 1\end{bmatrix}
  \begin{bmatrix}\mu \\  \tau_2 \\ \tau_3 \end{bmatrix}
  +
  \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \epsilon_4 \\ \epsilon_5 \\ \epsilon_6 \\ \epsilon_7 \end{bmatrix}
\end{equation}
In this model $\mu$ is the mean of the reference group and $\tau_i$ is the difference from group $i$ to the reference group. $\tau_1$ and is not included in the matrix because its difference from the reference group (itself) is necessarily zero.
\subsection{Example: Program Effectiveness}

Using data from Spector and Mazzeo (1980), we estimate a linear regression model with statsmodels.

\PyImg "regSpector.py" (p \pageref{py:regSpector}): Estimation of a linear regression model using the Spector and Mazzeo (1980) data set.
\index{python}{linear regression model}

\section{Assumptions}
Standard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variables and their relationship.  Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely.  Some methods are general enough that they can relax multiple assumptions at once, and in other cases this can be achieved by combining different extensions.  Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to get an accurate model.

The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares):

\begin{itemize}
  \item \textbf{Weak exogeneity}.  This essentially means that the predictor variables $x$ can be treated as fixed values, rather than random variables.  This means, for example, that the predictor variables are assumed to be error-free, that is they are not contaminated with measurement errors. Although not realistic in many settings, dropping this assumption leads to significantly more difficult errors-in-variables models.

  \item \textbf{Linearity}.  This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables.  Note that this assumption is much less restrictive than it may at first seem.  Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters.  The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently.  This trick is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given rank) of a predictor variable. This makes linear regression an extremely powerful inference method.  In fact, models such as polynomial regression are often "too powerful", in that they tend to overfit the data.  As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process.  Common examples are ridge regression and lasso regression.  Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, ridge regression and lasso regression can both be viewed as special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.)

  \item \textbf{Constant variance} (aka \emph{homoscedasticity}).  This means that different response variables have the same variance in their errors, regardless of the values of the predictor variables. In practice this assumption is invalid (i.e. the errors are heteroscedastic) if the response variables can vary over a wide scale. In order to determine for heterogeneous error variance, or when a pattern of residuals violates model assumptions of homoscedasticity (error is equally variable around the 'best-fitting line' for all points of x), it is prudent to look for a "fanning effect" between residual error and predicted values. This is to say there will be a systematic change in the absolute or squared residuals when plotted against the predicting outcome. Error will not be evenly distributed across the regression line. Heteroscedasticity will result in the averaging over of distinguishable variances around the points to get a single variance that is inaccurately representing all the variances of the line. In effect, residuals appear clustered and spread apart on their predicted plots for larger and smaller values for points along the linear regression line, and the mean squared error for the model will be wrong. Typically, for example, a response variable whose mean is large will have a greater variance than one whose mean is small. For example, a given person whose income is predicted to be \$100,000 may easily have an actual income of \$80,000 or \$120,000 (a standard deviation]] of around \$20,000), while another person with a predicted income of \$10,000 is unlikely to have the same \$20,000 standard deviation, which would imply their actual income would vary anywhere between -\$10,000 and \$30,000. (In fact, as this shows, in many cases – often the same cases where the assumption of normally distributed errors fails – the variance or standard deviation should be predicted to be proportional to the mean, rather than constant.) Simple linear regression estimation methods give less precise parameter estimates and misleading inferential quantities such as standard errors when substantial heteroscedasticity is present. However, various estimation techniques (e.g. weighted least squares and heteroscedasticity-consistent standard errors) can handle heteroscedasticity in a quite general way. Bayesian linear regression techniques can also be used when the variance is assumed to be a function of the mean. It is also possible in some cases to fix the problem by applying a transformation to the response variable (e.g. fit the logarithm of the response variable using a linear regression model, which implies that the response variable has a log-normal distribution rather than a normal distribution).

  \item \textbf{Independence of errors}.  This assumes that the errors of the response variables are uncorrelated with each other. (Actual statistical independence is a stronger condition than mere lack of correlation and is often not needed, although it can be exploited if it is known to hold.) Some methods (e.g. generalized least squares) are capable of handling correlated errors, although they typically require significantly more data unless some sort of regularization is used to bias the model towards assuming uncorrelated errors. Bayesian linear regression is a general way of handling this issue.

  \item \textbf{Lack of multicollinearity in the predictors}.  For standard least squares estimation methods, the design matrix $X$ must have full column rank $p$; otherwise, we have a condition known as multicollinearity in the predictor variables.  This can be triggered by having two or more perfectly correlated predictor variables (e.g. if the same predictor variable is mistakenly given twice, either without transforming one of the copies or by transforming one of the copies linearly). It can also happen if there is too little data available compared to the number of parameters to be estimated (e.g. fewer data points than regression coefficients). In the case of multicollinearity, the parameter vector $\beta$ will be non-identifiable, it has no unique solution.  At most we will be able to identify some of the parameters, i.e. narrow down its value to some linear subspace of $R^p$. Methods for fitting linear models with multicollinearity have been developed. Note that the more computationally expensive iterated algorithms for parameter estimation, such as those used in generalized linear models, do not suffer from this problem — and in fact it's quite normal to when handling categorical data|categorically-valued predictors to introduce a separate indicator variable predictor for each possible category, which inevitably introduces multicollinearity.
\end{itemize}
Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods:

\begin{itemize}
  \item The statistical relationship between the error terms and the regressors plays an important role in determining whether an estimation procedure has desirable sampling properties such as being unbiased and consistent.
  \item The arrangement, or probability distribution of the predictor variables $x$ has a major influence on the precision of estimates of $\beta$. Sampling and design of experiments are highly-developed subfields of statistics that provide guidance for collecting data in such a way to achieve a precise estimate of $\beta$.
\end{itemize}

\subsection{Interpretation}

A fitted linear regression model can be used to identify the relationship between a single predictor variable $x_j$ and the response variable $y$ when all the other predictor variables in the model are “held fixed”. Specifically, the interpretation of $\beta_j$ is the expected change in $y$ for a one-unit change in $x_j$ when the other covariates are held fixed—that is, the expected value of the partial derivative of $y$ with respect to $x_j$. This is sometimes called the ''unique effect'' of $x_j$ on ''y''. In contrast, the ''marginal effect'' of $x_j$ on $y$ can be assessed using a correlation coefficient or simple linear regression model relating $x_j$ to $y$; this effect is the total derivative of $y$ with respect to $x_j$.

Care must be taken when interpreting regression results, as some of the regressors may not allow for marginal changes (such as dummy variables, or the intercept term), while others cannot be held fixed (recall the example from the introduction: it would be impossible to “hold $t_j$ fixed” and at the same time change the value of $t_i^2$.

It is possible that the unique effect can be nearly zero even when the marginal effect is large. This may imply that some other covariate captures all the information in $x_j$, so that once that variable is in the model, there is no contribution of $x_j$ to the variation in $y$. Conversely, the unique effect of $x_j$ can be large while its marginal effect is nearly zero. This would happen if the other covariates explained a great deal of the variation of $y$, but they mainly explain variation in a way that is complementary to what is captured by $x_j$. In this case, including the other variables in the model reduces the part of the variability of $y$ that is unrelated to $x_j$, thereby strengthening the apparent relationship with $x_j$.

The meaning of the expression “held fixed” may depend on how the values of the predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been “held fixed” by the experimenter. Alternatively, the expression “held fixed” can refer to a selection that takes place in the context of data analysis. In this case, we “hold a variable fixed” by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of “held fixed” that can be used in an observational study.

The notion of a “unique effect” is appealing when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that is linked to the value of a predictor variable. However, it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design.

\PyImg "modeling.py" (p \pageref{py:modeling}) shows an example.
\index{python}{modeling}

\section{Evaluation of for Linear Regression Models}

One of the things that intimidated me about statistical modeling is the
deluge of expressions that appear when you run a modeling command. In the
following I will try to explain the most common parameters that you are
going to encounter when working with Linear Regression Models.


\subsection{Definitions for Regression with Intercept}

$n$ is the number of observations, $p$ is the number of regression parameters. For example, if you fit a straight line, $p=2$.
In the following $\hat{y}_i$ will indicate the fitted model values, and $\bar{y}$ will indicate the mean.

\begin{itemize}
  \item $SSM = \sum_{i=1}^n (\hat{y}_i-\bar{y})^2$ is the \emph{Sum of Square for Model}, or the sum of squares for the regression.
  \item $SSE = \sum_{i=1}^n (y_i-\hat{y}_i)^2$ is the \emph{sum of Squares for Error}, or the sum of squares for the residuals.
  \item $SST = \sum_{i=1}^n (y_i-\bar{y})^2$ is the \emph{Sum of Squares Total}, and is equivalent to the sample variance multiplied by $n-1$.
\end{itemize}

For multiple regression models, $SSM + SSE = SST$

\begin{itemize}
  \item $DFM = p - 1$ is the \emph{(Corrected) Degrees of Freedom for Model}. (The "-1" comes from the fact that we are only interested in the correlation, not in the absolute offset of the data.
  \item $DFE = n - p$ is the \emph{Degrees of Freedom for Error}
  \item $DFT = n - 1$ is the \emph{(Corrected) Degrees of Freedom Total}. The Horizontal line regression is the null hypothesis model.
\end{itemize}

For multiple regression models with intercept, DFM + DFE = DFT.

\begin{itemize}
  \item $MSM = SSM / DFM$ : \emph{Mean of Squares for Model}
  \item $MSE = SSE / DFE$ : \emph{Mean of Squares for Error}. MSE is an unbiased estimate for $\sigma^2$ for multiple regression models.
  \item $MST = SST / DFT$ : \emph{Mean of Squares Total}, which is the sample variance of the y-variable.
\end{itemize}

\subsection{The $R^2$ Value}

The $R^2$ value indicates the proportion of variation in the y-variable that is due to variation in the x-variables. For simple linear regression, the $R^2$ value is the square of the sample correlation $r_{xy}$. For multiple linear regression with intercept (which includes simple linear regression), the $R^2$ value is defined as

\begin{equation}
  R^2 = \frac{SSM}{SST}
\end{equation}

Many researchers prefer the adjusted $\bar{R}^2$ value (Eq \ref{eq:adjustedR2}), which is penalized for having a large number of parameters in the model:

Here is the logic behind the definition of $\bar{R}^2$:
$R^2$ is defined as $R^2 = 1 - SSE/SST$ or $1 - R^2 = SSE/SST$. To take into account the number of regression parameters $p$, define the adjusted R-squared value as
\begin{equation}
  1- \bar{R}^2 = \frac{Variance for Error}{Variance Total}
\end{equation}

where \emph{Variance for Error }is estimated by $SSE/DFE = SSE/(n-p)$, and \emph{Variance Total }is estimated by $SST/DFT = SST/(n-1)$. Thus,
\begin{eqnarray}
    1 - \bar{R}^2 &=& \frac{SSE/(n - p)}{SST/(n - 1)} \\
          	&=& \frac{SSE}{SST}\frac{n - 1}{n - p}
\end{eqnarray}

    so
\begin{eqnarray}
  \bar{R}^2 &=& 1 - \frac{SSE}{SST} \frac{n - 1}{(n - p} \\
    &=& 1 - (1 - R^2)\frac{n - 1}{n - p}
\end{eqnarray}


\subsection{The F-test}

If $t_1, t_2, ... , t_m$ are independent, $N(0, \sigma^2)$ random variables, then $\sum_{i=1}^m t_i^2$ is a $\chi^2$ (chi-squared) random variable with $m$ degrees of freedom.

For a multiple regression model with intercept,
\begin{equation}
  Y_j = \alpha + \beta_1 X_{1j} + ... + \beta_n X_{nj} + \epsilon_i = \alpha + \sum_{i=1}^n \beta_i X_{ij} + \epsilon_j = E(Y_j | X) + \epsilon_j
\end{equation}

we want to test the following null hypothesis and alternative hypothesis:

        $H_0$:   $\beta_1$ = $\beta_2$ = , ... , = $\beta_n$ = 0

        $H_1$:   $\beta_j \neq 0$, for at least one value of j

This test is known as the overall \emph{F-test for regression}.

It can be shown that if $H_0$ is true and the residuals are unbiased, homoscedastic, independent, and normal:

\begin{enumerate}
  \item $SSE / \sigma^2$ has a $\chi^2$ distribution with DFE degrees of freedom.
  \item $SSM / \sigma^2$ has a $\chi^2$ distribution with DFM degrees of freedom.
  \item SSE and SSM are independent random variables.
\end{enumerate}

If $u$ is a $\chi^2$ random variable with $n$ degrees of freedom, $v$ is a $\chi^2$ random variable with $m$ degrees of freedom, and $u$ and $v$ are independent, then if $F = \frac{u/n}{v/m}$ has an F distribution with $(n,m)$ degrees of freedom.

If H0 is true,

\begin{equation}
  F = \frac{(SSM/\sigma^2)/DFM}{(SSE/\sigma^2)/DFE} = \frac{SSM/DFM}{SSE/DFE} = \frac{MSM}{MSE},
\end{equation}

has an F distribution with $(DFM, DFE)$ degrees of freedom, and is independent of $\sigma$.


\subsection{Log-Likelihood Function}

A very common approach in statistics is the idea of \emph{Maximum Likelihood} estimation. \index{general}{Maximum likelihood} The basic idea is quite different from the \emph{minimum square} approach: there, the model is constant, and the errors of the response are variable; in contrast, in the maximum likelihood approach, the data response values are regarded as constant, and the likelihood of the model is maximised.

For the Classical Linear Regression Model (with normal errors) we have

\begin{equation}
  \epsilon = y_i - \sum_{k=1}^n \beta_k x_{ik} = y_i - \mathbf{x_i^{'}} \cdot \boldsymbol\beta \; in \; N(0, \sigma^2)
\end{equation}

so the probability density is given by

\begin{equation}
  p(\epsilon_i) =  \Phi (\frac{y_i - \mathbf{x_i^{'}} \cdot \boldsymbol\beta}{\sigma})
\end{equation}

where $\Phi(z)$ is the standard normal probability distribution function. The probability of independent samples is the product of the individual probabilities

\begin{equation}
  \Pi_{total} = \prod_{i=1}^n p(\epsilon_i)
\end{equation}

The \emph{Log Likelihood function} \index{general}{log likelihood function} is defined as

\begin{eqnarray*}
  Log L &=& log(\Pi_{total}) \\
  &=& log\left[\prod_{i=1}^n \frac{1}{\sigma\sqrt{2 \pi}} \exp \left(\frac{(y_i - \mathbf{x_i^{'}} \cdot \boldsymbol\beta)^2}{2 \sigma^2}\right)\right] \\
  &=& \sum_{i=1}^n\left[log\left(\frac{1}{\sigma \sqrt{2 \pi}}\right)- \left(\frac{(y_i - \mathbf{x_i^{'}} \cdot \boldsymbol\beta)^2}{2 \sigma^2}\right)\right] \\
 &=& n log(\sigma) - n log \sqrt{2 \pi} - \frac{SSE}{2 \sigma^2}
\end{eqnarray*}

It can be shown that the maximum likelihood estimator of $\sigma^2$ is

\begin{equation}
  E(\sigma^2) = \frac{SEE}{n}
\end{equation}

With this, the maximised log likelihood is
\begin{equation}
  Max(Log L) = -\frac{n}{2} log \left(\frac{2 \pi}{n} \right) - \frac{n}{2} - \frac{n}{2} log(SSE)
\end{equation}

and the maximised likelihood is

\begin{equation}
  Max(L) = \left( \frac{2 \pi}{n} \right)^{n/2} \cdot exp(- \frac{n}{2}) \cdot (SSE)^{-n/2}
\end{equation}

\subsection{Information Content of Statistical Models}

To judge the quality of your model, you should first visually inspect the residuals. In addition, you can also use a number of numerical criteria to assess the quality of a statistical model. These criteria represent various approaches for balancing model accuracy with parsimony.

We have already encountered the $adjusted R^2$ value (Eq \ref{eq:adjustedR2}), which - in contrast to the $R^2$ value - decreases if there are too many regressors in the model.

The \emph{Akaike Information Criterion AIC} \index{general}{Akaike Information Criterion AIC}

\begin{equation}
  AIC = n * ln(SSE / n) + 2p
\end{equation}

and the Schwartz or \emph{Bayesian Information Criterion BIC} \index{general}{Bayesian Information Criterion BIC}

\begin{equation}
  BIC = n * ln(SSE/n) + p ln(n)
\end{equation}

are other commonly encountered criteria.

\subsubsection{Analysis of Residuals}

The \textsf{OLS} command from \textsf{statsmodels.formula.api} provides some additional information about the residuals of the model:
\begin{description}
  \item[Omnibus]     In Multiple Regression the omnibus test is an ANOVA F test on all the coefficients, that is equivalent to the multiple correlations R Square F test. The omnibus F test is an overall test that examines model fit, thus rejecting the null hypothesis implies that the suggested linear model is not significally suitable to the data.
  \item[Skewness] Sample skewness of the residuals, i.e. if they have a tail to the left or to the right. Equivalent to \textsf{stats.skew(model.resid, bias=True)}.
  \item[Kurtosis] Sample kurtosis of the residuals, i.e. the pointedness of the data distribution. For normally distributed data approximately 3. (Equivalent to \textsf{stats.kurtosis(model.resid, fisher=False, bias=True)})
  \item[Durbin-Watson] A test statistic used to detect the presence of autocorrelation (a relationship between values separated from each other by a given time lag) in the residuals.
  \item[Jarque-Bera] A goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution.
\end{description}

\section{Bootstrapping} \index{general}{bootstrapping}

Another type of modelling is \emph{bootstrapping}/. Sometimes you have data describing a distribution, but do not know what type of distribution it is. So what can you do if you want to find out e.g. confidence values for the mean?

The answer is bootstrapping. Bootstrapping is a scheme of \emph{resampling}, i.e. taking additional samples repeatedly from the initial sample, to provide estimates of its variability. In a case where the distribution of the initial sample is unknown, bootstrapping is of especial help in that it provides information about the distribution.

\PyImg "bootstrapDemo.py" (p \pageref{py:bootstrapDemo}): Example of bootstrapping the confidence interval for the mean of a sample distribution.
\index{python}{bootstrapDemo}
